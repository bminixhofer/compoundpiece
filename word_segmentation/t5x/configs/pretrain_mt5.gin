from t5x import partitioning

include 'word_segmentation/t5x/configs/pretrain.gin'
include '../t5x/t5x/examples/t5/mt5/base.gin'

BATCH_SIZE = 512
MIXTURE_OR_TASK_NAME = "word_segmentation_mt5_pretrain"
TASK_FEATURE_LENGTHS = {'inputs': 64, 'targets': 64}
DROPOUT_RATE = 0.1
USE_CACHED_TASKS = False

# `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained
# using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be
# set to `pretraining batch_size` * `target_token_length`. For T5 and T5.1.1:
# `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.
LOSS_NORMALIZING_FACTOR = 234496

partitioning.PjitPartitioner.num_partitions = 4